{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636e6685",
   "metadata": {},
   "source": [
    "# Daraz Data Baker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5006024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "236dbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class DarazDataBaker:\n",
    "    \"\"\"\n",
    "        this class will be responsible for robust scraping data from the daraz officilal website, \n",
    "        save the data on the disk and \n",
    "        prepare the data for easy use \n",
    "    \"\"\"\n",
    "    def __init__(self, path=\"./\"):\n",
    "        self.data_instances = {} # holds the data object\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            self.path = path\n",
    "        else:\n",
    "            print(\"error! invalid path\")\n",
    "        \n",
    "        \n",
    "    def show_data_instances(self):\n",
    "        \"\"\" returns the name of all the data instances \"\"\"\n",
    "        return self.data_instances.keys()\n",
    "    \n",
    "    \n",
    "    def run_scrapper(self, query=\"nuts\", num_pages=-1, resume=True, save=True):\n",
    "        \"\"\"\n",
    "            - it is the heard of this class which is responsible for scraping data and storing \n",
    "            - into disk\n",
    "            parameters:\n",
    "                * query : (str) search query,\n",
    "                * last_page: (int) scrap at most up to that page, if -1 then scrap next page \n",
    "                * resume: (bool) determine whether scrapping should be \n",
    "                          resume or completely state from the begining for the given query\n",
    "                * save: (bool) determine whether the data should be store on the disk or not\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "\n",
    "        def get_current_page_number(query):\n",
    "            data_json = self.__load_page_context(os.path.join(self.path, \"page_context.json\"), query)\n",
    "            current_page = data_json[query][\"current_page\"]\n",
    "            return current_page\n",
    "        \n",
    "        current_page = get_current_page_number(query) if resume else 0\n",
    "        print(f\"resume : {resume}  current page: {current_page}\")\n",
    "        \n",
    "        last_page = current_page+1 if num_pages == -1 else current_page+num_pages\n",
    "            \n",
    "        if current_page >= last_page:\n",
    "            print(\"page already scrapped, please change the number\\\n",
    "            of last page greater than current page\")\n",
    "            return \n",
    "        \n",
    "        # scraping the data\n",
    "     \n",
    "        scraped_data_file = self.__get_fp(os.path.join(self.path,query+\".json\"),\"r+\", default_value=\"[]\")            \n",
    "        page_context_file = self.__get_fp(os.path.join(self.path,\"page_context.json\"), \"r+\")\n",
    "\n",
    "        # loading old data from disk\n",
    "        #move file point to the beginning\n",
    "        scraped_data = json.load(scraped_data_file)\n",
    "        print(scraped_data)\n",
    "        scraped_data_file.seek(0)\n",
    "        \n",
    "        page_context = json.load(page_context_file)\n",
    "        scraped_data_file.seek(0)\n",
    "        \n",
    "        #-----------helper functions---------------------------\n",
    "        import requests\n",
    "        import re\n",
    "        def quantity_parser(name):\n",
    "            # normalizing the text\n",
    "            name = ' '.join(name.split()).lower()\n",
    "            amount_pattern  = re.compile(\"([0-9]+ [a-z]+|[0-9]+[a-z]+)\")\n",
    "            find = amount_pattern.search(name)\n",
    "            amount = find.group(1)\n",
    "\n",
    "            price_unit = re.compile(\"([0-9]+)\\ ?([a-z]+)\")\n",
    "            find = price_unit.search(amount)\n",
    "            return {\"qty\": find.group(1),\"qty_unit\": find.group(2)}\n",
    "        \n",
    "        def name_parser(name):\n",
    "            # normalizing the text\n",
    "            name = ' '.join(name.split()).lower()\n",
    "\n",
    "            hyphen_search = re.compile(\"-\\ ?[0-9]+\").search(name)\n",
    "            hyphen_sub_str = hyphen_search.group() if hyphen_search else ''\n",
    "\n",
    "            hyphen_index = name.find(hyphen_sub_str) \n",
    "\n",
    "            by_index = name.find(\"by\")\n",
    "\n",
    "            num_search = re.compile(\"[0-9]+\").search(name)\n",
    "            num_sub_str = num_search.group() if num_search else ''\n",
    "\n",
    "            num_index = name.find(num_sub_str)\n",
    "            indexes = []\n",
    "            for num in [hyphen_index, by_index, num_index]:\n",
    "                if num != -1 and num != 0:\n",
    "                    indexes.append(num)\n",
    "\n",
    "            index = min(indexes) if len(indexes) > 0 else None\n",
    "            return {\"product_name\":name[:index].strip()}\n",
    "        \n",
    "        def parse_response(response):\n",
    "            interested_keys = [\"name\",\"nid\", \"image\", \"price\",\"ratingScore\",\"review\", \"location\", \"brandId\",\"brandName\",\"sellerId\",\"sellerName\"]\n",
    "            filtered_product_data = []\n",
    "            for item in json.loads(response.text)[\"mods\"][\"listItems\"]:\n",
    "                filtered_item = {} \n",
    "                for key in interested_keys:\n",
    "                    filtered_item[key] = item[key]\n",
    "                    if key == \"name\":\n",
    "                        name = item[key]\n",
    "                        filtered_item.update(quantity_parser(name))\n",
    "                        filtered_item.update(name_parser(name))\n",
    "\n",
    "                filtered_product_data.append(filtered_item)\n",
    "            return filtered_product_data\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        page_pointer = page_context[query][\"current_page\"]\n",
    "        for i in range(current_page, last_page):\n",
    "            # preparing api\n",
    "            end_point_api = f\"https://www.daraz.com.np/groceries-canned-dry-packaged-food-dried-goods-dried-fruit-nuts/?ajax=true&from=input&page={i}&q={query}\"\n",
    "            #updating page number\n",
    "            page_pointer += 1\n",
    "            print(f\"scraping page :{ page_pointer }\")\n",
    "            \n",
    "            # scraping new data\n",
    "            response = requests.get(end_point_api) \n",
    "            print(response.status_code)\n",
    "            if response.status_code == 200:\n",
    "                # updating old data\n",
    "                scraped_data += parse_response(response)\n",
    "                #saving updated data\n",
    "                json.dump(scraped_data, scraped_data_file)\n",
    "                #saving updated page context\n",
    "                page_context[query][\"current_page\"] = page_pointer\n",
    "                json.dump(page_context, page_context_file)\n",
    "                \n",
    "        print(\"scraper ran successfully\")\n",
    "            #         except Exception as e:\n",
    "            #             print(\"!exception: \",e)\n",
    "            #         finally:\n",
    "        scraped_data_file.close()\n",
    "        page_context_file.close()\n",
    "            \n",
    "    def __load_page_context(self, file_path, query=None):\n",
    "        import json\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path,\"r\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            print(f\"{file_path} doesnot exist! creating a new file with initial values\")\n",
    "            data = {\n",
    "                query:{\n",
    "                    \"current_page\":0\n",
    "                }\n",
    "            }\n",
    "\n",
    "            with open(file_path,\"w\") as f:\n",
    "                json.dump(data, f)\n",
    "                print(\"file created\")\n",
    "            return data\n",
    "\n",
    "    def __get_fp(self,file_name, mode=\"r+\", default_value=''):\n",
    "        if not os.path.exists(file_name):\n",
    "            #create a file with initial value\n",
    "            with open(file_name,\"w\") as f:\n",
    "                f.write(default_value)\n",
    "                \n",
    "        return open(file_name,mode)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "00d1782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = DarazDataBaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "68b38933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./page_context.json doesnot exist! creating a new file with initial values\n",
      "file created\n",
      "resume : True  current page: 0\n",
      "[]\n",
      "scraping page :1\n",
      "200\n",
      "scraping page :2\n",
      "200\n",
      "scraper ran successfully\n"
     ]
    }
   ],
   "source": [
    "engine.run_scrapper(num_pages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae00e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(x,y):\n",
    "    return x+y\n",
    "\n",
    "class Name:\n",
    "    def __init__(self, x,y):\n",
    "        self.x = x\n",
    "        self.y = y    \n",
    "    def show(self):\n",
    "        return sum(self.x,self.y)\n",
    "    \n",
    "    def get_x(self):\n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9244ff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Name(1,2)\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fb3a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cabc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
